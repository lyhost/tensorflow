// Code generated by protoc-gen-go. DO NOT EDIT.
// source: tensorflow/core/protobuf/tpu/optimization_parameters.proto

package protobuf

import (
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	wrappers "github.com/golang/protobuf/ptypes/wrappers"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

// if UNSPECIFIED (default), gradient accumulation is ENABLED.
type GradientAccumulationStatus_Status int32

const (
	GradientAccumulationStatus_UNSPECIFIED GradientAccumulationStatus_Status = 0
	GradientAccumulationStatus_ENABLED     GradientAccumulationStatus_Status = 1
	GradientAccumulationStatus_DISABLED    GradientAccumulationStatus_Status = 2
)

var GradientAccumulationStatus_Status_name = map[int32]string{
	0: "UNSPECIFIED",
	1: "ENABLED",
	2: "DISABLED",
}

var GradientAccumulationStatus_Status_value = map[string]int32{
	"UNSPECIFIED": 0,
	"ENABLED":     1,
	"DISABLED":    2,
}

func (x GradientAccumulationStatus_Status) String() string {
	return proto.EnumName(GradientAccumulationStatus_Status_name, int32(x))
}

func (GradientAccumulationStatus_Status) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{14, 0}
}

// Whether to enable or disable hot ID optimization.
// If UNSPECIFIED (default), hot ID optimization is DISABLED.
type HotIdReplicationConfiguration_Status int32

const (
	HotIdReplicationConfiguration_UNSPECIFIED HotIdReplicationConfiguration_Status = 0
	HotIdReplicationConfiguration_ENABLED     HotIdReplicationConfiguration_Status = 1
	HotIdReplicationConfiguration_DISABLED    HotIdReplicationConfiguration_Status = 2
)

var HotIdReplicationConfiguration_Status_name = map[int32]string{
	0: "UNSPECIFIED",
	1: "ENABLED",
	2: "DISABLED",
}

var HotIdReplicationConfiguration_Status_value = map[string]int32{
	"UNSPECIFIED": 0,
	"ENABLED":     1,
	"DISABLED":    2,
}

func (x HotIdReplicationConfiguration_Status) String() string {
	return proto.EnumName(HotIdReplicationConfiguration_Status_name, int32(x))
}

func (HotIdReplicationConfiguration_Status) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{15, 0}
}

type ClippingLimits struct {
	Lower                *wrappers.FloatValue `protobuf:"bytes,1,opt,name=lower,proto3" json:"lower,omitempty"`
	Upper                *wrappers.FloatValue `protobuf:"bytes,2,opt,name=upper,proto3" json:"upper,omitempty"`
	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
	XXX_unrecognized     []byte               `json:"-"`
	XXX_sizecache        int32                `json:"-"`
}

func (m *ClippingLimits) Reset()         { *m = ClippingLimits{} }
func (m *ClippingLimits) String() string { return proto.CompactTextString(m) }
func (*ClippingLimits) ProtoMessage()    {}
func (*ClippingLimits) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{0}
}

func (m *ClippingLimits) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ClippingLimits.Unmarshal(m, b)
}
func (m *ClippingLimits) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ClippingLimits.Marshal(b, m, deterministic)
}
func (m *ClippingLimits) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ClippingLimits.Merge(m, src)
}
func (m *ClippingLimits) XXX_Size() int {
	return xxx_messageInfo_ClippingLimits.Size(m)
}
func (m *ClippingLimits) XXX_DiscardUnknown() {
	xxx_messageInfo_ClippingLimits.DiscardUnknown(m)
}

var xxx_messageInfo_ClippingLimits proto.InternalMessageInfo

func (m *ClippingLimits) GetLower() *wrappers.FloatValue {
	if m != nil {
		return m.Lower
	}
	return nil
}

func (m *ClippingLimits) GetUpper() *wrappers.FloatValue {
	if m != nil {
		return m.Upper
	}
	return nil
}

// Dynamic learning rate specification in the TPUEmbeddingConfiguration. The
// actual learning rates are provided as a scalar input list to the
// SendTPUEmbeddingGradients Op indexed by their tag specified through the
// following proto.
type DynamicLearningRate struct {
	// For tables where learning rates are dynamically computed and communicated
	// to the TPU embedding program, a tag must be specified for the learning
	// rate.
	//
	// The tag must be a non-negative  integer. The total number of unique tags
	// must be less than or equal to the number of tables in the TPU embedding
	// configuration (a table does not specify any tag if it uses a constant
	// learning rate, and specifies exactly one tag if it uses dynamic learning
	// rates).
	//
	// All tags in the range [0, number_of_unique_tags) must be present in the TPU
	// embedding configuration, i.e. a tag cannot be skipped if a different tag
	// numerically greater than it is used in the configuration.
	//
	// If multiple tables specify the same tag, they *MUST* have
	// the same dynamic learning rate, for example, their dynamic learning rate
	// could be computed by the same TensorFlow sub-graph. The partitioning of the
	// embedding layer would be more optimal if the number_of_unique_tags is as
	// *LOW* as possible, i.e., if many tables share the same tag.
	//
	// The learning_rate input of the SendTPUEmbeddingGradients op is used to
	// communicate dynamic learning rates to the TPU embedding program.
	// The learning_rate input is a list of scalars where the size of the list is
	// equal to the number of unique tags. The learning rate associated with a
	// particular tag is specified by populating its corresponding index in the
	// list of learning_rate scalars.
	Tag                  int32    `protobuf:"varint,1,opt,name=tag,proto3" json:"tag,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *DynamicLearningRate) Reset()         { *m = DynamicLearningRate{} }
func (m *DynamicLearningRate) String() string { return proto.CompactTextString(m) }
func (*DynamicLearningRate) ProtoMessage()    {}
func (*DynamicLearningRate) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{1}
}

func (m *DynamicLearningRate) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_DynamicLearningRate.Unmarshal(m, b)
}
func (m *DynamicLearningRate) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_DynamicLearningRate.Marshal(b, m, deterministic)
}
func (m *DynamicLearningRate) XXX_Merge(src proto.Message) {
	xxx_messageInfo_DynamicLearningRate.Merge(m, src)
}
func (m *DynamicLearningRate) XXX_Size() int {
	return xxx_messageInfo_DynamicLearningRate.Size(m)
}
func (m *DynamicLearningRate) XXX_DiscardUnknown() {
	xxx_messageInfo_DynamicLearningRate.DiscardUnknown(m)
}

var xxx_messageInfo_DynamicLearningRate proto.InternalMessageInfo

func (m *DynamicLearningRate) GetTag() int32 {
	if m != nil {
		return m.Tag
	}
	return 0
}

// Source of learning rate to use.
type LearningRate struct {
	// Types that are valid to be assigned to LearningRate:
	//	*LearningRate_Constant
	//	*LearningRate_Dynamic
	LearningRate         isLearningRate_LearningRate `protobuf_oneof:"learning_rate"`
	XXX_NoUnkeyedLiteral struct{}                    `json:"-"`
	XXX_unrecognized     []byte                      `json:"-"`
	XXX_sizecache        int32                       `json:"-"`
}

func (m *LearningRate) Reset()         { *m = LearningRate{} }
func (m *LearningRate) String() string { return proto.CompactTextString(m) }
func (*LearningRate) ProtoMessage()    {}
func (*LearningRate) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{2}
}

func (m *LearningRate) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_LearningRate.Unmarshal(m, b)
}
func (m *LearningRate) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_LearningRate.Marshal(b, m, deterministic)
}
func (m *LearningRate) XXX_Merge(src proto.Message) {
	xxx_messageInfo_LearningRate.Merge(m, src)
}
func (m *LearningRate) XXX_Size() int {
	return xxx_messageInfo_LearningRate.Size(m)
}
func (m *LearningRate) XXX_DiscardUnknown() {
	xxx_messageInfo_LearningRate.DiscardUnknown(m)
}

var xxx_messageInfo_LearningRate proto.InternalMessageInfo

type isLearningRate_LearningRate interface {
	isLearningRate_LearningRate()
}

type LearningRate_Constant struct {
	Constant float32 `protobuf:"fixed32,1,opt,name=constant,proto3,oneof"`
}

type LearningRate_Dynamic struct {
	Dynamic *DynamicLearningRate `protobuf:"bytes,2,opt,name=dynamic,proto3,oneof"`
}

func (*LearningRate_Constant) isLearningRate_LearningRate() {}

func (*LearningRate_Dynamic) isLearningRate_LearningRate() {}

func (m *LearningRate) GetLearningRate() isLearningRate_LearningRate {
	if m != nil {
		return m.LearningRate
	}
	return nil
}

func (m *LearningRate) GetConstant() float32 {
	if x, ok := m.GetLearningRate().(*LearningRate_Constant); ok {
		return x.Constant
	}
	return 0
}

func (m *LearningRate) GetDynamic() *DynamicLearningRate {
	if x, ok := m.GetLearningRate().(*LearningRate_Dynamic); ok {
		return x.Dynamic
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*LearningRate) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*LearningRate_Constant)(nil),
		(*LearningRate_Dynamic)(nil),
	}
}

// https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L151
type AdagradParameters struct {
	InitialAccumulator   float32  `protobuf:"fixed32,1,opt,name=initial_accumulator,json=initialAccumulator,proto3" json:"initial_accumulator,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AdagradParameters) Reset()         { *m = AdagradParameters{} }
func (m *AdagradParameters) String() string { return proto.CompactTextString(m) }
func (*AdagradParameters) ProtoMessage()    {}
func (*AdagradParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{3}
}

func (m *AdagradParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_AdagradParameters.Unmarshal(m, b)
}
func (m *AdagradParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_AdagradParameters.Marshal(b, m, deterministic)
}
func (m *AdagradParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AdagradParameters.Merge(m, src)
}
func (m *AdagradParameters) XXX_Size() int {
	return xxx_messageInfo_AdagradParameters.Size(m)
}
func (m *AdagradParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_AdagradParameters.DiscardUnknown(m)
}

var xxx_messageInfo_AdagradParameters proto.InternalMessageInfo

func (m *AdagradParameters) GetInitialAccumulator() float32 {
	if m != nil {
		return m.InitialAccumulator
	}
	return 0
}

// Algorithm in http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.
type BoundedAdagradParameters struct {
	// Whether to use the updated or the old value of the accumulator when
	// computing the effective learning rate. When update_accumulator_first is set
	// to True, the updated value of the accumulator is used.
	UpdateAccumulatorFirst bool `protobuf:"varint,1,opt,name=update_accumulator_first,json=updateAccumulatorFirst,proto3" json:"update_accumulator_first,omitempty"`
	// The max_var_update value to use. Set value to 0 (default) to disable using
	// max_var_update to clip the gradient.
	MaxVarUpdate float32 `protobuf:"fixed32,2,opt,name=max_var_update,json=maxVarUpdate,proto3" json:"max_var_update,omitempty"`
	// The maximum value of the accumulator. Set max_accumulator to 0 (default)
	// to disable using max_accumulator to clip the accumulator.
	MaxAccumulator       float32  `protobuf:"fixed32,3,opt,name=max_accumulator,json=maxAccumulator,proto3" json:"max_accumulator,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *BoundedAdagradParameters) Reset()         { *m = BoundedAdagradParameters{} }
func (m *BoundedAdagradParameters) String() string { return proto.CompactTextString(m) }
func (*BoundedAdagradParameters) ProtoMessage()    {}
func (*BoundedAdagradParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{4}
}

func (m *BoundedAdagradParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_BoundedAdagradParameters.Unmarshal(m, b)
}
func (m *BoundedAdagradParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_BoundedAdagradParameters.Marshal(b, m, deterministic)
}
func (m *BoundedAdagradParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_BoundedAdagradParameters.Merge(m, src)
}
func (m *BoundedAdagradParameters) XXX_Size() int {
	return xxx_messageInfo_BoundedAdagradParameters.Size(m)
}
func (m *BoundedAdagradParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_BoundedAdagradParameters.DiscardUnknown(m)
}

var xxx_messageInfo_BoundedAdagradParameters proto.InternalMessageInfo

func (m *BoundedAdagradParameters) GetUpdateAccumulatorFirst() bool {
	if m != nil {
		return m.UpdateAccumulatorFirst
	}
	return false
}

func (m *BoundedAdagradParameters) GetMaxVarUpdate() float32 {
	if m != nil {
		return m.MaxVarUpdate
	}
	return 0
}

func (m *BoundedAdagradParameters) GetMaxAccumulator() float32 {
	if m != nil {
		return m.MaxAccumulator
	}
	return 0
}

// https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L423
type StochasticGradientDescentParameters struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StochasticGradientDescentParameters) Reset()         { *m = StochasticGradientDescentParameters{} }
func (m *StochasticGradientDescentParameters) String() string { return proto.CompactTextString(m) }
func (*StochasticGradientDescentParameters) ProtoMessage()    {}
func (*StochasticGradientDescentParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{5}
}

func (m *StochasticGradientDescentParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StochasticGradientDescentParameters.Unmarshal(m, b)
}
func (m *StochasticGradientDescentParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StochasticGradientDescentParameters.Marshal(b, m, deterministic)
}
func (m *StochasticGradientDescentParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StochasticGradientDescentParameters.Merge(m, src)
}
func (m *StochasticGradientDescentParameters) XXX_Size() int {
	return xxx_messageInfo_StochasticGradientDescentParameters.Size(m)
}
func (m *StochasticGradientDescentParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_StochasticGradientDescentParameters.DiscardUnknown(m)
}

var xxx_messageInfo_StochasticGradientDescentParameters proto.InternalMessageInfo

// https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L192
type FtrlParameters struct {
	L1                   float32  `protobuf:"fixed32,1,opt,name=l1,proto3" json:"l1,omitempty"`
	L2                   float32  `protobuf:"fixed32,2,opt,name=l2,proto3" json:"l2,omitempty"`
	LrPower              float32  `protobuf:"fixed32,3,opt,name=lr_power,json=lrPower,proto3" json:"lr_power,omitempty"`
	InitialAccum         float32  `protobuf:"fixed32,4,opt,name=initial_accum,json=initialAccum,proto3" json:"initial_accum,omitempty"`
	InitialLinear        float32  `protobuf:"fixed32,5,opt,name=initial_linear,json=initialLinear,proto3" json:"initial_linear,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *FtrlParameters) Reset()         { *m = FtrlParameters{} }
func (m *FtrlParameters) String() string { return proto.CompactTextString(m) }
func (*FtrlParameters) ProtoMessage()    {}
func (*FtrlParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{6}
}

func (m *FtrlParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_FtrlParameters.Unmarshal(m, b)
}
func (m *FtrlParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_FtrlParameters.Marshal(b, m, deterministic)
}
func (m *FtrlParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_FtrlParameters.Merge(m, src)
}
func (m *FtrlParameters) XXX_Size() int {
	return xxx_messageInfo_FtrlParameters.Size(m)
}
func (m *FtrlParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_FtrlParameters.DiscardUnknown(m)
}

var xxx_messageInfo_FtrlParameters proto.InternalMessageInfo

func (m *FtrlParameters) GetL1() float32 {
	if m != nil {
		return m.L1
	}
	return 0
}

func (m *FtrlParameters) GetL2() float32 {
	if m != nil {
		return m.L2
	}
	return 0
}

func (m *FtrlParameters) GetLrPower() float32 {
	if m != nil {
		return m.LrPower
	}
	return 0
}

func (m *FtrlParameters) GetInitialAccum() float32 {
	if m != nil {
		return m.InitialAccum
	}
	return 0
}

func (m *FtrlParameters) GetInitialLinear() float32 {
	if m != nil {
		return m.InitialLinear
	}
	return 0
}

// The Adam optimizer does not implement hyper-parameter update; use the dynamic
// learning rate feature instead, setting the learning rate to:
// user learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
// Here, t is the current timestep.
//
// https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer
// https://github.com/tensorflow/tensorflow/blob/ab51450c817674c8ff08a7ae4f8ac50cdc4bed8b/tensorflow/python/training/adam.py#L54
//
// Note that the code by default implements the lazy version of Adam
// (https://www.tensorflow.org/api_docs/python/tf/contrib/opt/LazyAdamOptimizer)
// unless the use_non_lazy_adam parameter is set, in which case it implements
// the normal version of Adam that updates all parameters in the embedding
// table, even for entries that are not used in the current minibatch
// (https://www.tensorflow.org/api_docs/python/tf/contrib/opt/AdamOptimizer). If
// use_non_lazy_adam is enabled, gradient accumulation is also required to be
// enabled in order to get correct results; a warning will be printed otherwise
// (which may change to an error in the future). If use_sum_inside_sqrt is set,
// the Adam variable update formula will be changed from m / (sqrt(v) + epsilon)
// to m / sqrt(v + epsilon**2); this option improves the performance of TPU
// training and is not expected to harm model quality.
type AdamParameters struct {
	Beta1                float32  `protobuf:"fixed32,3,opt,name=beta1,proto3" json:"beta1,omitempty"`
	Beta2                float32  `protobuf:"fixed32,4,opt,name=beta2,proto3" json:"beta2,omitempty"`
	Epsilon              float32  `protobuf:"fixed32,5,opt,name=epsilon,proto3" json:"epsilon,omitempty"`
	InitialM             float32  `protobuf:"fixed32,6,opt,name=initial_m,json=initialM,proto3" json:"initial_m,omitempty"`
	InitialV             float32  `protobuf:"fixed32,7,opt,name=initial_v,json=initialV,proto3" json:"initial_v,omitempty"`
	UseNonLazyAdam       bool     `protobuf:"varint,8,opt,name=use_non_lazy_adam,json=useNonLazyAdam,proto3" json:"use_non_lazy_adam,omitempty"`
	UseSumInsideSqrt     bool     `protobuf:"varint,10,opt,name=use_sum_inside_sqrt,json=useSumInsideSqrt,proto3" json:"use_sum_inside_sqrt,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AdamParameters) Reset()         { *m = AdamParameters{} }
func (m *AdamParameters) String() string { return proto.CompactTextString(m) }
func (*AdamParameters) ProtoMessage()    {}
func (*AdamParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{7}
}

func (m *AdamParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_AdamParameters.Unmarshal(m, b)
}
func (m *AdamParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_AdamParameters.Marshal(b, m, deterministic)
}
func (m *AdamParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AdamParameters.Merge(m, src)
}
func (m *AdamParameters) XXX_Size() int {
	return xxx_messageInfo_AdamParameters.Size(m)
}
func (m *AdamParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_AdamParameters.DiscardUnknown(m)
}

var xxx_messageInfo_AdamParameters proto.InternalMessageInfo

func (m *AdamParameters) GetBeta1() float32 {
	if m != nil {
		return m.Beta1
	}
	return 0
}

func (m *AdamParameters) GetBeta2() float32 {
	if m != nil {
		return m.Beta2
	}
	return 0
}

func (m *AdamParameters) GetEpsilon() float32 {
	if m != nil {
		return m.Epsilon
	}
	return 0
}

func (m *AdamParameters) GetInitialM() float32 {
	if m != nil {
		return m.InitialM
	}
	return 0
}

func (m *AdamParameters) GetInitialV() float32 {
	if m != nil {
		return m.InitialV
	}
	return 0
}

func (m *AdamParameters) GetUseNonLazyAdam() bool {
	if m != nil {
		return m.UseNonLazyAdam
	}
	return false
}

func (m *AdamParameters) GetUseSumInsideSqrt() bool {
	if m != nil {
		return m.UseSumInsideSqrt
	}
	return false
}

// https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L271
type MomentumParameters struct {
	Momentum             float32  `protobuf:"fixed32,1,opt,name=momentum,proto3" json:"momentum,omitempty"`
	UseNesterov          bool     `protobuf:"varint,2,opt,name=use_nesterov,json=useNesterov,proto3" json:"use_nesterov,omitempty"`
	InitialAccum         float32  `protobuf:"fixed32,3,opt,name=initial_accum,json=initialAccum,proto3" json:"initial_accum,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *MomentumParameters) Reset()         { *m = MomentumParameters{} }
func (m *MomentumParameters) String() string { return proto.CompactTextString(m) }
func (*MomentumParameters) ProtoMessage()    {}
func (*MomentumParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{8}
}

func (m *MomentumParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_MomentumParameters.Unmarshal(m, b)
}
func (m *MomentumParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_MomentumParameters.Marshal(b, m, deterministic)
}
func (m *MomentumParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MomentumParameters.Merge(m, src)
}
func (m *MomentumParameters) XXX_Size() int {
	return xxx_messageInfo_MomentumParameters.Size(m)
}
func (m *MomentumParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_MomentumParameters.DiscardUnknown(m)
}

var xxx_messageInfo_MomentumParameters proto.InternalMessageInfo

func (m *MomentumParameters) GetMomentum() float32 {
	if m != nil {
		return m.Momentum
	}
	return 0
}

func (m *MomentumParameters) GetUseNesterov() bool {
	if m != nil {
		return m.UseNesterov
	}
	return false
}

func (m *MomentumParameters) GetInitialAccum() float32 {
	if m != nil {
		return m.InitialAccum
	}
	return 0
}

// https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L356
type RmsPropParameters struct {
	Rho                  float32  `protobuf:"fixed32,1,opt,name=rho,proto3" json:"rho,omitempty"`
	Momentum             float32  `protobuf:"fixed32,2,opt,name=momentum,proto3" json:"momentum,omitempty"`
	Epsilon              float32  `protobuf:"fixed32,3,opt,name=epsilon,proto3" json:"epsilon,omitempty"`
	InitialMs            float32  `protobuf:"fixed32,4,opt,name=initial_ms,json=initialMs,proto3" json:"initial_ms,omitempty"`
	InitialMom           float32  `protobuf:"fixed32,5,opt,name=initial_mom,json=initialMom,proto3" json:"initial_mom,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RmsPropParameters) Reset()         { *m = RmsPropParameters{} }
func (m *RmsPropParameters) String() string { return proto.CompactTextString(m) }
func (*RmsPropParameters) ProtoMessage()    {}
func (*RmsPropParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{9}
}

func (m *RmsPropParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RmsPropParameters.Unmarshal(m, b)
}
func (m *RmsPropParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RmsPropParameters.Marshal(b, m, deterministic)
}
func (m *RmsPropParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RmsPropParameters.Merge(m, src)
}
func (m *RmsPropParameters) XXX_Size() int {
	return xxx_messageInfo_RmsPropParameters.Size(m)
}
func (m *RmsPropParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_RmsPropParameters.DiscardUnknown(m)
}

var xxx_messageInfo_RmsPropParameters proto.InternalMessageInfo

func (m *RmsPropParameters) GetRho() float32 {
	if m != nil {
		return m.Rho
	}
	return 0
}

func (m *RmsPropParameters) GetMomentum() float32 {
	if m != nil {
		return m.Momentum
	}
	return 0
}

func (m *RmsPropParameters) GetEpsilon() float32 {
	if m != nil {
		return m.Epsilon
	}
	return 0
}

func (m *RmsPropParameters) GetInitialMs() float32 {
	if m != nil {
		return m.InitialMs
	}
	return 0
}

func (m *RmsPropParameters) GetInitialMom() float32 {
	if m != nil {
		return m.InitialMom
	}
	return 0
}

// https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L372
type CenteredRmsPropParameters struct {
	Rho                  float32  `protobuf:"fixed32,1,opt,name=rho,proto3" json:"rho,omitempty"`
	Momentum             float32  `protobuf:"fixed32,2,opt,name=momentum,proto3" json:"momentum,omitempty"`
	Epsilon              float32  `protobuf:"fixed32,3,opt,name=epsilon,proto3" json:"epsilon,omitempty"`
	InitialMs            float32  `protobuf:"fixed32,4,opt,name=initial_ms,json=initialMs,proto3" json:"initial_ms,omitempty"`
	InitialMom           float32  `protobuf:"fixed32,5,opt,name=initial_mom,json=initialMom,proto3" json:"initial_mom,omitempty"`
	InitialMg            float32  `protobuf:"fixed32,6,opt,name=initial_mg,json=initialMg,proto3" json:"initial_mg,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CenteredRmsPropParameters) Reset()         { *m = CenteredRmsPropParameters{} }
func (m *CenteredRmsPropParameters) String() string { return proto.CompactTextString(m) }
func (*CenteredRmsPropParameters) ProtoMessage()    {}
func (*CenteredRmsPropParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{10}
}

func (m *CenteredRmsPropParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CenteredRmsPropParameters.Unmarshal(m, b)
}
func (m *CenteredRmsPropParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CenteredRmsPropParameters.Marshal(b, m, deterministic)
}
func (m *CenteredRmsPropParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CenteredRmsPropParameters.Merge(m, src)
}
func (m *CenteredRmsPropParameters) XXX_Size() int {
	return xxx_messageInfo_CenteredRmsPropParameters.Size(m)
}
func (m *CenteredRmsPropParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_CenteredRmsPropParameters.DiscardUnknown(m)
}

var xxx_messageInfo_CenteredRmsPropParameters proto.InternalMessageInfo

func (m *CenteredRmsPropParameters) GetRho() float32 {
	if m != nil {
		return m.Rho
	}
	return 0
}

func (m *CenteredRmsPropParameters) GetMomentum() float32 {
	if m != nil {
		return m.Momentum
	}
	return 0
}

func (m *CenteredRmsPropParameters) GetEpsilon() float32 {
	if m != nil {
		return m.Epsilon
	}
	return 0
}

func (m *CenteredRmsPropParameters) GetInitialMs() float32 {
	if m != nil {
		return m.InitialMs
	}
	return 0
}

func (m *CenteredRmsPropParameters) GetInitialMom() float32 {
	if m != nil {
		return m.InitialMom
	}
	return 0
}

func (m *CenteredRmsPropParameters) GetInitialMg() float32 {
	if m != nil {
		return m.InitialMg
	}
	return 0
}

// Variant of algorithm in http://proceedings.mlr.press/v44/shamir15.pdf
type MdlAdagradLightParameters struct {
	L2                    float32  `protobuf:"fixed32,1,opt,name=l2,proto3" json:"l2,omitempty"`
	LrPower               float32  `protobuf:"fixed32,2,opt,name=lr_power,json=lrPower,proto3" json:"lr_power,omitempty"`
	MinServableMdlBenefit float32  `protobuf:"fixed32,3,opt,name=min_servable_mdl_benefit,json=minServableMdlBenefit,proto3" json:"min_servable_mdl_benefit,omitempty"`
	MdlMixInMargin        float32  `protobuf:"fixed32,4,opt,name=mdl_mix_in_margin,json=mdlMixInMargin,proto3" json:"mdl_mix_in_margin,omitempty"`
	MdlBenefitRampupCoeff float32  `protobuf:"fixed32,5,opt,name=mdl_benefit_rampup_coeff,json=mdlBenefitRampupCoeff,proto3" json:"mdl_benefit_rampup_coeff,omitempty"`
	MdlMinWeight          float32  `protobuf:"fixed32,6,opt,name=mdl_min_weight,json=mdlMinWeight,proto3" json:"mdl_min_weight,omitempty"`
	BenefitRevisitScale   float32  `protobuf:"fixed32,7,opt,name=benefit_revisit_scale,json=benefitRevisitScale,proto3" json:"benefit_revisit_scale,omitempty"`
	MaxEventBenefit       float32  `protobuf:"fixed32,8,opt,name=max_event_benefit,json=maxEventBenefit,proto3" json:"max_event_benefit,omitempty"`
	MaxTotalBenefit       float32  `protobuf:"fixed32,9,opt,name=max_total_benefit,json=maxTotalBenefit,proto3" json:"max_total_benefit,omitempty"`
	MdlHardLimit          float32  `protobuf:"fixed32,10,opt,name=mdl_hard_limit,json=mdlHardLimit,proto3" json:"mdl_hard_limit,omitempty"`
	HardLimitMinBenefit   bool     `protobuf:"varint,11,opt,name=hard_limit_min_benefit,json=hardLimitMinBenefit,proto3" json:"hard_limit_min_benefit,omitempty"`
	MdlRegularize         bool     `protobuf:"varint,12,opt,name=mdl_regularize,json=mdlRegularize,proto3" json:"mdl_regularize,omitempty"`
	InitialAccumulator    float32  `protobuf:"fixed32,13,opt,name=initial_accumulator,json=initialAccumulator,proto3" json:"initial_accumulator,omitempty"`
	InitialWeight         float32  `protobuf:"fixed32,14,opt,name=initial_weight,json=initialWeight,proto3" json:"initial_weight,omitempty"`
	InitialBenefit        float32  `protobuf:"fixed32,15,opt,name=initial_benefit,json=initialBenefit,proto3" json:"initial_benefit,omitempty"`
	XXX_NoUnkeyedLiteral  struct{} `json:"-"`
	XXX_unrecognized      []byte   `json:"-"`
	XXX_sizecache         int32    `json:"-"`
}

func (m *MdlAdagradLightParameters) Reset()         { *m = MdlAdagradLightParameters{} }
func (m *MdlAdagradLightParameters) String() string { return proto.CompactTextString(m) }
func (*MdlAdagradLightParameters) ProtoMessage()    {}
func (*MdlAdagradLightParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{11}
}

func (m *MdlAdagradLightParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_MdlAdagradLightParameters.Unmarshal(m, b)
}
func (m *MdlAdagradLightParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_MdlAdagradLightParameters.Marshal(b, m, deterministic)
}
func (m *MdlAdagradLightParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MdlAdagradLightParameters.Merge(m, src)
}
func (m *MdlAdagradLightParameters) XXX_Size() int {
	return xxx_messageInfo_MdlAdagradLightParameters.Size(m)
}
func (m *MdlAdagradLightParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_MdlAdagradLightParameters.DiscardUnknown(m)
}

var xxx_messageInfo_MdlAdagradLightParameters proto.InternalMessageInfo

func (m *MdlAdagradLightParameters) GetL2() float32 {
	if m != nil {
		return m.L2
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetLrPower() float32 {
	if m != nil {
		return m.LrPower
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMinServableMdlBenefit() float32 {
	if m != nil {
		return m.MinServableMdlBenefit
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMdlMixInMargin() float32 {
	if m != nil {
		return m.MdlMixInMargin
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMdlBenefitRampupCoeff() float32 {
	if m != nil {
		return m.MdlBenefitRampupCoeff
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMdlMinWeight() float32 {
	if m != nil {
		return m.MdlMinWeight
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetBenefitRevisitScale() float32 {
	if m != nil {
		return m.BenefitRevisitScale
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMaxEventBenefit() float32 {
	if m != nil {
		return m.MaxEventBenefit
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMaxTotalBenefit() float32 {
	if m != nil {
		return m.MaxTotalBenefit
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetMdlHardLimit() float32 {
	if m != nil {
		return m.MdlHardLimit
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetHardLimitMinBenefit() bool {
	if m != nil {
		return m.HardLimitMinBenefit
	}
	return false
}

func (m *MdlAdagradLightParameters) GetMdlRegularize() bool {
	if m != nil {
		return m.MdlRegularize
	}
	return false
}

func (m *MdlAdagradLightParameters) GetInitialAccumulator() float32 {
	if m != nil {
		return m.InitialAccumulator
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetInitialWeight() float32 {
	if m != nil {
		return m.InitialWeight
	}
	return 0
}

func (m *MdlAdagradLightParameters) GetInitialBenefit() float32 {
	if m != nil {
		return m.InitialBenefit
	}
	return 0
}

// https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L68
type AdadeltaParameters struct {
	Rho                  float32  `protobuf:"fixed32,1,opt,name=rho,proto3" json:"rho,omitempty"`
	Epsilon              float32  `protobuf:"fixed32,2,opt,name=epsilon,proto3" json:"epsilon,omitempty"`
	InitialAccumulator   float32  `protobuf:"fixed32,3,opt,name=initial_accumulator,json=initialAccumulator,proto3" json:"initial_accumulator,omitempty"`
	InitialUpdate        float32  `protobuf:"fixed32,4,opt,name=initial_update,json=initialUpdate,proto3" json:"initial_update,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AdadeltaParameters) Reset()         { *m = AdadeltaParameters{} }
func (m *AdadeltaParameters) String() string { return proto.CompactTextString(m) }
func (*AdadeltaParameters) ProtoMessage()    {}
func (*AdadeltaParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{12}
}

func (m *AdadeltaParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_AdadeltaParameters.Unmarshal(m, b)
}
func (m *AdadeltaParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_AdadeltaParameters.Marshal(b, m, deterministic)
}
func (m *AdadeltaParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AdadeltaParameters.Merge(m, src)
}
func (m *AdadeltaParameters) XXX_Size() int {
	return xxx_messageInfo_AdadeltaParameters.Size(m)
}
func (m *AdadeltaParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_AdadeltaParameters.DiscardUnknown(m)
}

var xxx_messageInfo_AdadeltaParameters proto.InternalMessageInfo

func (m *AdadeltaParameters) GetRho() float32 {
	if m != nil {
		return m.Rho
	}
	return 0
}

func (m *AdadeltaParameters) GetEpsilon() float32 {
	if m != nil {
		return m.Epsilon
	}
	return 0
}

func (m *AdadeltaParameters) GetInitialAccumulator() float32 {
	if m != nil {
		return m.InitialAccumulator
	}
	return 0
}

func (m *AdadeltaParameters) GetInitialUpdate() float32 {
	if m != nil {
		return m.InitialUpdate
	}
	return 0
}

// https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer
// https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/training_ops.cc#L164
type ProximalAdagradParameters struct {
	L1                   float32  `protobuf:"fixed32,1,opt,name=l1,proto3" json:"l1,omitempty"`
	L2                   float32  `protobuf:"fixed32,2,opt,name=l2,proto3" json:"l2,omitempty"`
	InitialAccumulator   float32  `protobuf:"fixed32,3,opt,name=initial_accumulator,json=initialAccumulator,proto3" json:"initial_accumulator,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ProximalAdagradParameters) Reset()         { *m = ProximalAdagradParameters{} }
func (m *ProximalAdagradParameters) String() string { return proto.CompactTextString(m) }
func (*ProximalAdagradParameters) ProtoMessage()    {}
func (*ProximalAdagradParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{13}
}

func (m *ProximalAdagradParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProximalAdagradParameters.Unmarshal(m, b)
}
func (m *ProximalAdagradParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProximalAdagradParameters.Marshal(b, m, deterministic)
}
func (m *ProximalAdagradParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProximalAdagradParameters.Merge(m, src)
}
func (m *ProximalAdagradParameters) XXX_Size() int {
	return xxx_messageInfo_ProximalAdagradParameters.Size(m)
}
func (m *ProximalAdagradParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_ProximalAdagradParameters.DiscardUnknown(m)
}

var xxx_messageInfo_ProximalAdagradParameters proto.InternalMessageInfo

func (m *ProximalAdagradParameters) GetL1() float32 {
	if m != nil {
		return m.L1
	}
	return 0
}

func (m *ProximalAdagradParameters) GetL2() float32 {
	if m != nil {
		return m.L2
	}
	return 0
}

func (m *ProximalAdagradParameters) GetInitialAccumulator() float32 {
	if m != nil {
		return m.InitialAccumulator
	}
	return 0
}

// Status of using gradient accumulation (doing two passes over the input
// gradients: one to accumulate them into a temporary array and another to apply
// them using the actual optimization algorithm). The extra message is to wrap
// the enum for scoping.
type GradientAccumulationStatus struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *GradientAccumulationStatus) Reset()         { *m = GradientAccumulationStatus{} }
func (m *GradientAccumulationStatus) String() string { return proto.CompactTextString(m) }
func (*GradientAccumulationStatus) ProtoMessage()    {}
func (*GradientAccumulationStatus) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{14}
}

func (m *GradientAccumulationStatus) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_GradientAccumulationStatus.Unmarshal(m, b)
}
func (m *GradientAccumulationStatus) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_GradientAccumulationStatus.Marshal(b, m, deterministic)
}
func (m *GradientAccumulationStatus) XXX_Merge(src proto.Message) {
	xxx_messageInfo_GradientAccumulationStatus.Merge(m, src)
}
func (m *GradientAccumulationStatus) XXX_Size() int {
	return xxx_messageInfo_GradientAccumulationStatus.Size(m)
}
func (m *GradientAccumulationStatus) XXX_DiscardUnknown() {
	xxx_messageInfo_GradientAccumulationStatus.DiscardUnknown(m)
}

var xxx_messageInfo_GradientAccumulationStatus proto.InternalMessageInfo

// Configuration proto for hot ID optimization. This is an experimental feature
// that is currently disabled (by default).
type HotIdReplicationConfiguration struct {
	Status               HotIdReplicationConfiguration_Status `protobuf:"varint,1,opt,name=status,proto3,enum=tensorflow.tpu.HotIdReplicationConfiguration_Status" json:"status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                             `json:"-"`
	XXX_unrecognized     []byte                               `json:"-"`
	XXX_sizecache        int32                                `json:"-"`
}

func (m *HotIdReplicationConfiguration) Reset()         { *m = HotIdReplicationConfiguration{} }
func (m *HotIdReplicationConfiguration) String() string { return proto.CompactTextString(m) }
func (*HotIdReplicationConfiguration) ProtoMessage()    {}
func (*HotIdReplicationConfiguration) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{15}
}

func (m *HotIdReplicationConfiguration) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_HotIdReplicationConfiguration.Unmarshal(m, b)
}
func (m *HotIdReplicationConfiguration) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_HotIdReplicationConfiguration.Marshal(b, m, deterministic)
}
func (m *HotIdReplicationConfiguration) XXX_Merge(src proto.Message) {
	xxx_messageInfo_HotIdReplicationConfiguration.Merge(m, src)
}
func (m *HotIdReplicationConfiguration) XXX_Size() int {
	return xxx_messageInfo_HotIdReplicationConfiguration.Size(m)
}
func (m *HotIdReplicationConfiguration) XXX_DiscardUnknown() {
	xxx_messageInfo_HotIdReplicationConfiguration.DiscardUnknown(m)
}

var xxx_messageInfo_HotIdReplicationConfiguration proto.InternalMessageInfo

func (m *HotIdReplicationConfiguration) GetStatus() HotIdReplicationConfiguration_Status {
	if m != nil {
		return m.Status
	}
	return HotIdReplicationConfiguration_UNSPECIFIED
}

type OptimizationParameters struct {
	// Learning rate used for updating the embedding layer parameters.
	LearningRate *LearningRate `protobuf:"bytes,13,opt,name=learning_rate,json=learningRate,proto3" json:"learning_rate,omitempty"`
	// Limits to which to clip the weight values after the backward pass; not
	// present means no limits are applied.
	ClippingLimits *ClippingLimits `protobuf:"bytes,2,opt,name=clipping_limits,json=clippingLimits,proto3" json:"clipping_limits,omitempty"`
	// Limits to which to clip the backward pass gradient before using it for
	// updates; not present means no limits are applied.
	GradientClippingLimits *ClippingLimits `protobuf:"bytes,7,opt,name=gradient_clipping_limits,json=gradientClippingLimits,proto3" json:"gradient_clipping_limits,omitempty"`
	// Amount of weight decay to apply; see weight_decay_optimizers.py for
	// details. Almost all optimizers are supported with this option (MDL Adagrad
	// Light does not work, and SGD does not behave as expected if it is enabled).
	// Although there is no check, users who want weight decay will probably also
	// want to enable gradient accumulation as well so that the decay will happen
	// once per minibatch.
	WeightDecayFactor float32 `protobuf:"fixed32,16,opt,name=weight_decay_factor,json=weightDecayFactor,proto3" json:"weight_decay_factor,omitempty"`
	// Status of using gradient accumulation (doing two passes over the input
	// gradients: one to accumulate them into a temporary array and another to
	// apply them using the actual optimization algorithm).
	GradientAccumulationStatus GradientAccumulationStatus_Status `protobuf:"varint,17,opt,name=gradient_accumulation_status,json=gradientAccumulationStatus,proto3,enum=tensorflow.tpu.GradientAccumulationStatus_Status" json:"gradient_accumulation_status,omitempty"`
	// Configuration proto for hot ID replication. This is an experimental
	// feature that is currently disabled (by default).
	HotIdReplicationConfiguration *HotIdReplicationConfiguration `protobuf:"bytes,18,opt,name=hot_id_replication_configuration,json=hotIdReplicationConfiguration,proto3" json:"hot_id_replication_configuration,omitempty"`
	// Optimization algorithm parameters; which field is selected determines which
	// algorithm to use.
	//
	// Types that are valid to be assigned to Parameters:
	//	*OptimizationParameters_Adagrad
	//	*OptimizationParameters_BoundedAdagrad
	//	*OptimizationParameters_StochasticGradientDescent
	//	*OptimizationParameters_Ftrl
	//	*OptimizationParameters_Adam
	//	*OptimizationParameters_Momentum
	//	*OptimizationParameters_RmsProp
	//	*OptimizationParameters_CenteredRmsProp
	//	*OptimizationParameters_MdlAdagradLight
	//	*OptimizationParameters_Adadelta
	//	*OptimizationParameters_ProximalAdagrad
	Parameters           isOptimizationParameters_Parameters `protobuf_oneof:"parameters"`
	XXX_NoUnkeyedLiteral struct{}                            `json:"-"`
	XXX_unrecognized     []byte                              `json:"-"`
	XXX_sizecache        int32                               `json:"-"`
}

func (m *OptimizationParameters) Reset()         { *m = OptimizationParameters{} }
func (m *OptimizationParameters) String() string { return proto.CompactTextString(m) }
func (*OptimizationParameters) ProtoMessage()    {}
func (*OptimizationParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{16}
}

func (m *OptimizationParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_OptimizationParameters.Unmarshal(m, b)
}
func (m *OptimizationParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_OptimizationParameters.Marshal(b, m, deterministic)
}
func (m *OptimizationParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_OptimizationParameters.Merge(m, src)
}
func (m *OptimizationParameters) XXX_Size() int {
	return xxx_messageInfo_OptimizationParameters.Size(m)
}
func (m *OptimizationParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_OptimizationParameters.DiscardUnknown(m)
}

var xxx_messageInfo_OptimizationParameters proto.InternalMessageInfo

func (m *OptimizationParameters) GetLearningRate() *LearningRate {
	if m != nil {
		return m.LearningRate
	}
	return nil
}

func (m *OptimizationParameters) GetClippingLimits() *ClippingLimits {
	if m != nil {
		return m.ClippingLimits
	}
	return nil
}

func (m *OptimizationParameters) GetGradientClippingLimits() *ClippingLimits {
	if m != nil {
		return m.GradientClippingLimits
	}
	return nil
}

func (m *OptimizationParameters) GetWeightDecayFactor() float32 {
	if m != nil {
		return m.WeightDecayFactor
	}
	return 0
}

func (m *OptimizationParameters) GetGradientAccumulationStatus() GradientAccumulationStatus_Status {
	if m != nil {
		return m.GradientAccumulationStatus
	}
	return GradientAccumulationStatus_UNSPECIFIED
}

func (m *OptimizationParameters) GetHotIdReplicationConfiguration() *HotIdReplicationConfiguration {
	if m != nil {
		return m.HotIdReplicationConfiguration
	}
	return nil
}

type isOptimizationParameters_Parameters interface {
	isOptimizationParameters_Parameters()
}

type OptimizationParameters_Adagrad struct {
	Adagrad *AdagradParameters `protobuf:"bytes,3,opt,name=adagrad,proto3,oneof"`
}

type OptimizationParameters_BoundedAdagrad struct {
	BoundedAdagrad *BoundedAdagradParameters `protobuf:"bytes,19,opt,name=bounded_adagrad,json=boundedAdagrad,proto3,oneof"`
}

type OptimizationParameters_StochasticGradientDescent struct {
	StochasticGradientDescent *StochasticGradientDescentParameters `protobuf:"bytes,4,opt,name=stochastic_gradient_descent,json=stochasticGradientDescent,proto3,oneof"`
}

type OptimizationParameters_Ftrl struct {
	Ftrl *FtrlParameters `protobuf:"bytes,5,opt,name=ftrl,proto3,oneof"`
}

type OptimizationParameters_Adam struct {
	Adam *AdamParameters `protobuf:"bytes,6,opt,name=adam,proto3,oneof"`
}

type OptimizationParameters_Momentum struct {
	Momentum *MomentumParameters `protobuf:"bytes,8,opt,name=momentum,proto3,oneof"`
}

type OptimizationParameters_RmsProp struct {
	RmsProp *RmsPropParameters `protobuf:"bytes,9,opt,name=rms_prop,json=rmsProp,proto3,oneof"`
}

type OptimizationParameters_CenteredRmsProp struct {
	CenteredRmsProp *CenteredRmsPropParameters `protobuf:"bytes,10,opt,name=centered_rms_prop,json=centeredRmsProp,proto3,oneof"`
}

type OptimizationParameters_MdlAdagradLight struct {
	MdlAdagradLight *MdlAdagradLightParameters `protobuf:"bytes,11,opt,name=mdl_adagrad_light,json=mdlAdagradLight,proto3,oneof"`
}

type OptimizationParameters_Adadelta struct {
	Adadelta *AdadeltaParameters `protobuf:"bytes,12,opt,name=adadelta,proto3,oneof"`
}

type OptimizationParameters_ProximalAdagrad struct {
	ProximalAdagrad *ProximalAdagradParameters `protobuf:"bytes,14,opt,name=proximal_adagrad,json=proximalAdagrad,proto3,oneof"`
}

func (*OptimizationParameters_Adagrad) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_BoundedAdagrad) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_StochasticGradientDescent) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_Ftrl) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_Adam) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_Momentum) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_RmsProp) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_CenteredRmsProp) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_MdlAdagradLight) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_Adadelta) isOptimizationParameters_Parameters() {}

func (*OptimizationParameters_ProximalAdagrad) isOptimizationParameters_Parameters() {}

func (m *OptimizationParameters) GetParameters() isOptimizationParameters_Parameters {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *OptimizationParameters) GetAdagrad() *AdagradParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_Adagrad); ok {
		return x.Adagrad
	}
	return nil
}

func (m *OptimizationParameters) GetBoundedAdagrad() *BoundedAdagradParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_BoundedAdagrad); ok {
		return x.BoundedAdagrad
	}
	return nil
}

func (m *OptimizationParameters) GetStochasticGradientDescent() *StochasticGradientDescentParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_StochasticGradientDescent); ok {
		return x.StochasticGradientDescent
	}
	return nil
}

func (m *OptimizationParameters) GetFtrl() *FtrlParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_Ftrl); ok {
		return x.Ftrl
	}
	return nil
}

func (m *OptimizationParameters) GetAdam() *AdamParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_Adam); ok {
		return x.Adam
	}
	return nil
}

func (m *OptimizationParameters) GetMomentum() *MomentumParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_Momentum); ok {
		return x.Momentum
	}
	return nil
}

func (m *OptimizationParameters) GetRmsProp() *RmsPropParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_RmsProp); ok {
		return x.RmsProp
	}
	return nil
}

func (m *OptimizationParameters) GetCenteredRmsProp() *CenteredRmsPropParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_CenteredRmsProp); ok {
		return x.CenteredRmsProp
	}
	return nil
}

func (m *OptimizationParameters) GetMdlAdagradLight() *MdlAdagradLightParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_MdlAdagradLight); ok {
		return x.MdlAdagradLight
	}
	return nil
}

func (m *OptimizationParameters) GetAdadelta() *AdadeltaParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_Adadelta); ok {
		return x.Adadelta
	}
	return nil
}

func (m *OptimizationParameters) GetProximalAdagrad() *ProximalAdagradParameters {
	if x, ok := m.GetParameters().(*OptimizationParameters_ProximalAdagrad); ok {
		return x.ProximalAdagrad
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*OptimizationParameters) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*OptimizationParameters_Adagrad)(nil),
		(*OptimizationParameters_BoundedAdagrad)(nil),
		(*OptimizationParameters_StochasticGradientDescent)(nil),
		(*OptimizationParameters_Ftrl)(nil),
		(*OptimizationParameters_Adam)(nil),
		(*OptimizationParameters_Momentum)(nil),
		(*OptimizationParameters_RmsProp)(nil),
		(*OptimizationParameters_CenteredRmsProp)(nil),
		(*OptimizationParameters_MdlAdagradLight)(nil),
		(*OptimizationParameters_Adadelta)(nil),
		(*OptimizationParameters_ProximalAdagrad)(nil),
	}
}

// Specification of an optimization algorithm's state variables (both the main
// value vector and any extra accumulators, etc.). This proto is only used
// internally by the TPU software and is not exposed directly to the TF model.
type StateVariableSpecification struct {
	// Parameter name for the state variable.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Usage type of this state variable.
	//
	// Types that are valid to be assigned to Usage:
	//	*StateVariableSpecification_UserDefined_
	//	*StateVariableSpecification_FillWithConstant_
	Usage                isStateVariableSpecification_Usage `protobuf_oneof:"usage"`
	XXX_NoUnkeyedLiteral struct{}                           `json:"-"`
	XXX_unrecognized     []byte                             `json:"-"`
	XXX_sizecache        int32                              `json:"-"`
}

func (m *StateVariableSpecification) Reset()         { *m = StateVariableSpecification{} }
func (m *StateVariableSpecification) String() string { return proto.CompactTextString(m) }
func (*StateVariableSpecification) ProtoMessage()    {}
func (*StateVariableSpecification) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{17}
}

func (m *StateVariableSpecification) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StateVariableSpecification.Unmarshal(m, b)
}
func (m *StateVariableSpecification) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StateVariableSpecification.Marshal(b, m, deterministic)
}
func (m *StateVariableSpecification) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StateVariableSpecification.Merge(m, src)
}
func (m *StateVariableSpecification) XXX_Size() int {
	return xxx_messageInfo_StateVariableSpecification.Size(m)
}
func (m *StateVariableSpecification) XXX_DiscardUnknown() {
	xxx_messageInfo_StateVariableSpecification.DiscardUnknown(m)
}

var xxx_messageInfo_StateVariableSpecification proto.InternalMessageInfo

func (m *StateVariableSpecification) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

type isStateVariableSpecification_Usage interface {
	isStateVariableSpecification_Usage()
}

type StateVariableSpecification_UserDefined_ struct {
	UserDefined *StateVariableSpecification_UserDefined `protobuf:"bytes,2,opt,name=user_defined,json=userDefined,proto3,oneof"`
}

type StateVariableSpecification_FillWithConstant_ struct {
	FillWithConstant *StateVariableSpecification_FillWithConstant `protobuf:"bytes,3,opt,name=fill_with_constant,json=fillWithConstant,proto3,oneof"`
}

func (*StateVariableSpecification_UserDefined_) isStateVariableSpecification_Usage() {}

func (*StateVariableSpecification_FillWithConstant_) isStateVariableSpecification_Usage() {}

func (m *StateVariableSpecification) GetUsage() isStateVariableSpecification_Usage {
	if m != nil {
		return m.Usage
	}
	return nil
}

func (m *StateVariableSpecification) GetUserDefined() *StateVariableSpecification_UserDefined {
	if x, ok := m.GetUsage().(*StateVariableSpecification_UserDefined_); ok {
		return x.UserDefined
	}
	return nil
}

func (m *StateVariableSpecification) GetFillWithConstant() *StateVariableSpecification_FillWithConstant {
	if x, ok := m.GetUsage().(*StateVariableSpecification_FillWithConstant_); ok {
		return x.FillWithConstant
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*StateVariableSpecification) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*StateVariableSpecification_UserDefined_)(nil),
		(*StateVariableSpecification_FillWithConstant_)(nil),
	}
}

// A normal state variable that should be saved and restored in checkpoints
// and used as an input or output to non-debug TensorFlow ops.
type StateVariableSpecification_UserDefined struct {
	// For padding embedding rows, this field specifies the initial value to be
	// used. Separate initial values need to be specified for the embeddings and
	// any extra accumulators. The initial values should be specified so as to
	// maintain two invariants during model training:
	// (1) The embedding vector multiplied by zero returns a vector containing
	//     all zeros. To maintain this invariant, the embedding values should
	//     never be NaNs or +-infinity.
	// (2) Repeatedly applying the optimizer using a gradient vector of all
	//     zeros does not cause the embeddings or slot variables to become NaNs
	//     or +-infinity.
	// The padding row is looked up when no embedding IDs are present for a
	// feature. The semantics of embedding lookup dictate that the output must
	// be zero under this scenario.
	PaddingInitialValue  float64  `protobuf:"fixed64,1,opt,name=padding_initial_value,json=paddingInitialValue,proto3" json:"padding_initial_value,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StateVariableSpecification_UserDefined) Reset() {
	*m = StateVariableSpecification_UserDefined{}
}
func (m *StateVariableSpecification_UserDefined) String() string { return proto.CompactTextString(m) }
func (*StateVariableSpecification_UserDefined) ProtoMessage()    {}
func (*StateVariableSpecification_UserDefined) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{17, 0}
}

func (m *StateVariableSpecification_UserDefined) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StateVariableSpecification_UserDefined.Unmarshal(m, b)
}
func (m *StateVariableSpecification_UserDefined) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StateVariableSpecification_UserDefined.Marshal(b, m, deterministic)
}
func (m *StateVariableSpecification_UserDefined) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StateVariableSpecification_UserDefined.Merge(m, src)
}
func (m *StateVariableSpecification_UserDefined) XXX_Size() int {
	return xxx_messageInfo_StateVariableSpecification_UserDefined.Size(m)
}
func (m *StateVariableSpecification_UserDefined) XXX_DiscardUnknown() {
	xxx_messageInfo_StateVariableSpecification_UserDefined.DiscardUnknown(m)
}

var xxx_messageInfo_StateVariableSpecification_UserDefined proto.InternalMessageInfo

func (m *StateVariableSpecification_UserDefined) GetPaddingInitialValue() float64 {
	if m != nil {
		return m.PaddingInitialValue
	}
	return 0
}

// A state variable that should be filled with a constant and normally hidden
// from users (used for intermediate gradients being accumulated, for
// example).
type StateVariableSpecification_FillWithConstant struct {
	InitialValue         float64  `protobuf:"fixed64,1,opt,name=initial_value,json=initialValue,proto3" json:"initial_value,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StateVariableSpecification_FillWithConstant) Reset() {
	*m = StateVariableSpecification_FillWithConstant{}
}
func (m *StateVariableSpecification_FillWithConstant) String() string {
	return proto.CompactTextString(m)
}
func (*StateVariableSpecification_FillWithConstant) ProtoMessage() {}
func (*StateVariableSpecification_FillWithConstant) Descriptor() ([]byte, []int) {
	return fileDescriptor_a641844a6aab9de2, []int{17, 1}
}

func (m *StateVariableSpecification_FillWithConstant) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StateVariableSpecification_FillWithConstant.Unmarshal(m, b)
}
func (m *StateVariableSpecification_FillWithConstant) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StateVariableSpecification_FillWithConstant.Marshal(b, m, deterministic)
}
func (m *StateVariableSpecification_FillWithConstant) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StateVariableSpecification_FillWithConstant.Merge(m, src)
}
func (m *StateVariableSpecification_FillWithConstant) XXX_Size() int {
	return xxx_messageInfo_StateVariableSpecification_FillWithConstant.Size(m)
}
func (m *StateVariableSpecification_FillWithConstant) XXX_DiscardUnknown() {
	xxx_messageInfo_StateVariableSpecification_FillWithConstant.DiscardUnknown(m)
}

var xxx_messageInfo_StateVariableSpecification_FillWithConstant proto.InternalMessageInfo

func (m *StateVariableSpecification_FillWithConstant) GetInitialValue() float64 {
	if m != nil {
		return m.InitialValue
	}
	return 0
}

func init() {
	proto.RegisterEnum("tensorflow.tpu.GradientAccumulationStatus_Status", GradientAccumulationStatus_Status_name, GradientAccumulationStatus_Status_value)
	proto.RegisterEnum("tensorflow.tpu.HotIdReplicationConfiguration_Status", HotIdReplicationConfiguration_Status_name, HotIdReplicationConfiguration_Status_value)
	proto.RegisterType((*ClippingLimits)(nil), "tensorflow.tpu.ClippingLimits")
	proto.RegisterType((*DynamicLearningRate)(nil), "tensorflow.tpu.DynamicLearningRate")
	proto.RegisterType((*LearningRate)(nil), "tensorflow.tpu.LearningRate")
	proto.RegisterType((*AdagradParameters)(nil), "tensorflow.tpu.AdagradParameters")
	proto.RegisterType((*BoundedAdagradParameters)(nil), "tensorflow.tpu.BoundedAdagradParameters")
	proto.RegisterType((*StochasticGradientDescentParameters)(nil), "tensorflow.tpu.StochasticGradientDescentParameters")
	proto.RegisterType((*FtrlParameters)(nil), "tensorflow.tpu.FtrlParameters")
	proto.RegisterType((*AdamParameters)(nil), "tensorflow.tpu.AdamParameters")
	proto.RegisterType((*MomentumParameters)(nil), "tensorflow.tpu.MomentumParameters")
	proto.RegisterType((*RmsPropParameters)(nil), "tensorflow.tpu.RmsPropParameters")
	proto.RegisterType((*CenteredRmsPropParameters)(nil), "tensorflow.tpu.CenteredRmsPropParameters")
	proto.RegisterType((*MdlAdagradLightParameters)(nil), "tensorflow.tpu.MdlAdagradLightParameters")
	proto.RegisterType((*AdadeltaParameters)(nil), "tensorflow.tpu.AdadeltaParameters")
	proto.RegisterType((*ProximalAdagradParameters)(nil), "tensorflow.tpu.ProximalAdagradParameters")
	proto.RegisterType((*GradientAccumulationStatus)(nil), "tensorflow.tpu.GradientAccumulationStatus")
	proto.RegisterType((*HotIdReplicationConfiguration)(nil), "tensorflow.tpu.HotIdReplicationConfiguration")
	proto.RegisterType((*OptimizationParameters)(nil), "tensorflow.tpu.OptimizationParameters")
	proto.RegisterType((*StateVariableSpecification)(nil), "tensorflow.tpu.StateVariableSpecification")
	proto.RegisterType((*StateVariableSpecification_UserDefined)(nil), "tensorflow.tpu.StateVariableSpecification.UserDefined")
	proto.RegisterType((*StateVariableSpecification_FillWithConstant)(nil), "tensorflow.tpu.StateVariableSpecification.FillWithConstant")
}

func init() {
	proto.RegisterFile("tensorflow/core/protobuf/tpu/optimization_parameters.proto", fileDescriptor_a641844a6aab9de2)
}

var fileDescriptor_a641844a6aab9de2 = []byte{
	// 1704 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xcc, 0x58, 0x5f, 0x73, 0xdb, 0xb8,
	0x11, 0xb7, 0xe4, 0x7f, 0xca, 0xca, 0x91, 0x64, 0xb8, 0xc9, 0xc8, 0x4e, 0x72, 0xbd, 0x53, 0x9a,
	0xc9, 0x5d, 0x67, 0x4e, 0x1e, 0x2b, 0x99, 0x5e, 0xa7, 0x7f, 0xae, 0xf5, 0xdf, 0x93, 0x6f, 0xac,
	0xd4, 0x43, 0x35, 0x4e, 0xa7, 0x7d, 0xc0, 0xc0, 0x24, 0x44, 0x61, 0x0a, 0x02, 0x3c, 0x10, 0xb4,
	0x9d, 0x3c, 0xf4, 0x6b, 0x34, 0x33, 0x7d, 0xec, 0x37, 0xe9, 0xc7, 0xe9, 0x47, 0xe8, 0x4b, 0x3b,
	0x00, 0x41, 0x9a, 0x94, 0xac, 0x34, 0xe9, 0x53, 0xdf, 0x88, 0xfd, 0xf3, 0xc3, 0xee, 0x62, 0x17,
	0xbb, 0x20, 0xfc, 0x42, 0x53, 0x91, 0x48, 0x35, 0xe1, 0xf2, 0x7a, 0xd7, 0x97, 0x8a, 0xee, 0xc6,
	0x4a, 0x6a, 0x79, 0x99, 0x4e, 0x76, 0x75, 0x9c, 0xee, 0xca, 0x58, 0xb3, 0x88, 0xbd, 0x23, 0x9a,
	0x49, 0x81, 0x63, 0xa2, 0x48, 0x44, 0x35, 0x55, 0x49, 0xdf, 0x0a, 0xa1, 0xd6, 0xad, 0x6e, 0x5f,
	0xc7, 0xe9, 0xce, 0x67, 0xa1, 0x94, 0x21, 0x2f, 0x41, 0x5c, 0x2b, 0x12, 0xc7, 0x85, 0x7c, 0xef,
	0x0a, 0x5a, 0x87, 0x9c, 0xc5, 0x31, 0x13, 0xe1, 0x19, 0x8b, 0x98, 0x4e, 0xd0, 0x1e, 0xac, 0x72,
	0x79, 0x4d, 0x55, 0xb7, 0xf6, 0x79, 0xed, 0xcb, 0xe6, 0xe0, 0x51, 0x3f, 0x43, 0xe8, 0xe7, 0x08,
	0xfd, 0x13, 0x2e, 0x89, 0xbe, 0x20, 0x3c, 0xa5, 0x5e, 0x26, 0x69, 0x54, 0x52, 0x03, 0xda, 0xad,
	0x7f, 0x84, 0x8a, 0x95, 0xec, 0x3d, 0x87, 0xad, 0xa3, 0xb7, 0x82, 0x44, 0xcc, 0x3f, 0xa3, 0x44,
	0x09, 0x26, 0x42, 0x8f, 0x68, 0x8a, 0x3a, 0xb0, 0xac, 0x49, 0x68, 0xb7, 0x5e, 0xf5, 0xcc, 0x67,
	0xef, 0x2f, 0xb0, 0x51, 0x91, 0x78, 0x0c, 0x0d, 0x5f, 0x8a, 0x44, 0x13, 0xa1, 0xad, 0x58, 0x7d,
	0xb8, 0xe4, 0x15, 0x14, 0xf4, 0x1b, 0x58, 0x0f, 0x32, 0x58, 0x67, 0xcb, 0xd3, 0x7e, 0x35, 0x20,
	0xfd, 0x3b, 0x76, 0x1d, 0x2e, 0x79, 0xb9, 0xd6, 0x41, 0x1b, 0xee, 0x73, 0xc7, 0xc2, 0x8a, 0x68,
	0xda, 0x3b, 0x82, 0xcd, 0xfd, 0x80, 0x84, 0x8a, 0x04, 0xe7, 0x45, 0xac, 0xd1, 0x2e, 0x6c, 0x31,
	0xc1, 0x34, 0x23, 0x1c, 0x13, 0xdf, 0x4f, 0xa3, 0x94, 0x13, 0x2d, 0xb3, 0x88, 0xd5, 0x3d, 0xe4,
	0x58, 0xfb, 0xb7, 0x9c, 0xde, 0xdf, 0x6b, 0xd0, 0x3d, 0x90, 0xa9, 0x08, 0x68, 0x30, 0x8f, 0xf6,
	0x73, 0xe8, 0xa6, 0x71, 0x40, 0x34, 0x2d, 0x83, 0xe1, 0x09, 0x53, 0x49, 0xe6, 0x62, 0xc3, 0x7b,
	0x98, 0xf1, 0x4b, 0x88, 0x27, 0x86, 0x8b, 0x7e, 0x02, 0xad, 0x88, 0xdc, 0xe0, 0x2b, 0xa2, 0x70,
	0x26, 0x61, 0xbd, 0xae, 0x7b, 0x1b, 0x11, 0xb9, 0xb9, 0x20, 0xea, 0xb5, 0xa5, 0xa1, 0xe7, 0xd0,
	0x36, 0x52, 0x65, 0x4b, 0x97, 0xad, 0x98, 0x51, 0x2e, 0x5b, 0xf9, 0x0c, 0x9e, 0x8e, 0xb5, 0xf4,
	0xa7, 0x24, 0xd1, 0xcc, 0xff, 0x4e, 0x91, 0x80, 0x51, 0xa1, 0x8f, 0x68, 0xe2, 0x53, 0xa1, 0x6f,
	0xed, 0xed, 0xfd, 0xb5, 0x06, 0xad, 0x13, 0xad, 0x78, 0xc9, 0x85, 0x16, 0xd4, 0xf9, 0x9e, 0xf3,
	0xbf, 0xce, 0xf7, 0xec, 0x7a, 0xe0, 0x8c, 0xa9, 0xf3, 0x01, 0xda, 0x86, 0x06, 0x57, 0x38, 0xb6,
	0x79, 0x95, 0xed, 0xbd, 0xce, 0xd5, 0xb9, 0x4d, 0x9e, 0xa7, 0x70, 0xbf, 0x12, 0xcb, 0xee, 0x4a,
	0xe6, 0x42, 0x39, 0x8a, 0xe8, 0x19, 0xb4, 0x72, 0x21, 0xce, 0x04, 0x25, 0xaa, 0xbb, 0x6a, 0xa5,
	0x72, 0xd5, 0x33, 0x4b, 0xec, 0xfd, 0xb3, 0x06, 0xad, 0xfd, 0x80, 0x44, 0x25, 0xcb, 0x7e, 0x04,
	0xab, 0x97, 0x54, 0x93, 0x3d, 0xb7, 0x6d, 0xb6, 0xc8, 0xa9, 0x03, 0xb7, 0x59, 0xb6, 0x40, 0x5d,
	0x58, 0xa7, 0x71, 0xc2, 0xb8, 0x14, 0x0e, 0x3e, 0x5f, 0xa2, 0x47, 0x70, 0x2f, 0xdf, 0x3f, 0xea,
	0xae, 0x59, 0x5e, 0xc3, 0x11, 0x46, 0x65, 0xe6, 0x55, 0x77, 0xbd, 0xc2, 0xbc, 0x40, 0x5f, 0xc1,
	0x66, 0x9a, 0x50, 0x2c, 0xa4, 0xc0, 0x9c, 0xbc, 0x7b, 0x8b, 0x49, 0x40, 0xa2, 0x6e, 0xc3, 0x9e,
	0x6a, 0x2b, 0x4d, 0xe8, 0x2b, 0x29, 0xce, 0xc8, 0xbb, 0xb7, 0xc6, 0x68, 0xf4, 0x35, 0x6c, 0x19,
	0xd1, 0x24, 0x8d, 0x30, 0x13, 0x09, 0x0b, 0x28, 0x4e, 0x7e, 0x50, 0xba, 0x0b, 0x56, 0xb8, 0x93,
	0x26, 0x74, 0x9c, 0x46, 0xa7, 0x96, 0x31, 0xfe, 0x41, 0xe9, 0xde, 0x0d, 0xa0, 0x91, 0x8c, 0xa8,
	0xd0, 0x69, 0xd9, 0xdf, 0x1d, 0x68, 0x44, 0x8e, 0xea, 0xce, 0xa3, 0x58, 0xa3, 0x2f, 0x60, 0xc3,
	0xda, 0x42, 0x13, 0x4d, 0x95, 0xbc, 0xb2, 0xe7, 0xd3, 0xf0, 0x9a, 0xc6, 0x0c, 0x47, 0x9a, 0x3f,
	0x8d, 0xe5, 0xf9, 0xd3, 0xe8, 0xfd, 0xad, 0x06, 0x9b, 0x5e, 0x94, 0x9c, 0x2b, 0x19, 0x97, 0x76,
	0xee, 0xc0, 0xb2, 0x9a, 0x4a, 0xb7, 0xa9, 0xf9, 0xac, 0xd8, 0x52, 0x9f, 0xb1, 0xa5, 0x14, 0xeb,
	0xe5, 0x6a, 0xac, 0x9f, 0x00, 0x14, 0xb1, 0x4e, 0xdc, 0x01, 0xe5, 0x01, 0x1e, 0x25, 0xe8, 0xc7,
	0xd0, 0x2c, 0xd8, 0x32, 0x72, 0x07, 0x95, 0x6b, 0x8c, 0x64, 0xd4, 0xfb, 0x47, 0x0d, 0xb6, 0x0f,
	0xa9, 0xd0, 0x54, 0xd1, 0xe0, 0xff, 0xd6, 0xca, 0x8a, 0x7e, 0xe8, 0x52, 0xaa, 0xd0, 0x0f, 0x7b,
	0xef, 0x57, 0x61, 0x7b, 0x14, 0x70, 0x77, 0x59, 0x9c, 0xb1, 0x70, 0xaa, 0x67, 0xca, 0x6d, 0x50,
	0x94, 0x5b, 0xb5, 0xbc, 0xea, 0xd5, 0xf2, 0xfa, 0x06, 0xba, 0x11, 0x13, 0x38, 0xa1, 0xea, 0x8a,
	0x5c, 0x72, 0x8a, 0xa3, 0x80, 0xe3, 0x4b, 0x2a, 0xe8, 0x84, 0x69, 0xe7, 0xd2, 0x83, 0x88, 0x89,
	0xb1, 0x63, 0x8f, 0x02, 0x7e, 0x90, 0x31, 0x4d, 0xe2, 0x1a, 0xd9, 0x88, 0xdd, 0x60, 0x26, 0x70,
	0x44, 0x54, 0xc8, 0x84, 0xf3, 0xb3, 0x15, 0x05, 0x7c, 0xc4, 0x6e, 0x4e, 0xc5, 0xc8, 0x52, 0xed,
	0x1e, 0xb7, 0xb0, 0x58, 0x91, 0x28, 0x4e, 0x63, 0xec, 0x4b, 0x3a, 0x99, 0x38, 0xcf, 0x1f, 0x44,
	0x05, 0xb0, 0x67, 0xb9, 0x87, 0x86, 0x69, 0xef, 0x2f, 0xbb, 0x87, 0xc0, 0xd7, 0xd4, 0xf8, 0xe8,
	0x02, 0xb1, 0x61, 0x37, 0x10, 0x6f, 0x2c, 0x0d, 0x0d, 0xe0, 0x41, 0x01, 0x4d, 0xaf, 0x58, 0xc2,
	0x34, 0x4e, 0x7c, 0xc2, 0xa9, 0xab, 0xb5, 0x2d, 0xc7, 0xf4, 0x32, 0xde, 0xd8, 0xb0, 0xd0, 0x4f,
	0x61, 0xd3, 0xdc, 0x79, 0xf4, 0x8a, 0x0a, 0x5d, 0xf8, 0xdb, 0xb0, 0xf2, 0xe6, 0x32, 0x3c, 0x36,
	0xf4, 0xdc, 0x53, 0x27, 0xab, 0xa5, 0x26, 0xb7, 0xb1, 0xb9, 0x57, 0xc8, 0xfe, 0xde, 0xd0, 0x73,
	0x59, 0x67, 0xf1, 0x94, 0xa8, 0x00, 0x73, 0xd3, 0x30, 0x6d, 0x79, 0x66, 0x16, 0x0f, 0x89, 0x0a,
	0x6c, 0x13, 0x45, 0x2f, 0xe0, 0xe1, 0xad, 0x84, 0x75, 0x2f, 0x87, 0x6d, 0xda, 0x92, 0xdb, 0x9a,
	0xe6, 0xa2, 0x23, 0x26, 0x72, 0xe8, 0x67, 0x19, 0xb4, 0xa2, 0x61, 0xca, 0x89, 0x62, 0xef, 0x68,
	0x77, 0xc3, 0x0a, 0xdf, 0x8f, 0x02, 0xee, 0x15, 0xc4, 0x45, 0xbd, 0xe7, 0xfe, 0xa2, 0xde, 0x53,
	0xbe, 0x3b, 0x5d, 0x90, 0x5b, 0x95, 0xbb, 0xd3, 0x45, 0xf9, 0x39, 0xb4, 0x73, 0xb1, 0xdc, 0xd8,
	0x76, 0x76, 0xda, 0x8e, 0xec, 0xec, 0xec, 0xbd, 0xaf, 0x01, 0xda, 0x0f, 0x48, 0x40, 0xb9, 0x26,
	0x1f, 0x2c, 0xac, 0x52, 0xf1, 0xd4, 0xab, 0xc5, 0xb3, 0xc0, 0x87, 0xe5, 0x8f, 0xf1, 0xc1, 0x35,
	0xba, 0x95, 0x8a, 0x0f, 0x59, 0xa7, 0xeb, 0x71, 0xd8, 0x3e, 0x57, 0xf2, 0x86, 0x45, 0x84, 0xcf,
	0xb7, 0xd9, 0xff, 0xd6, 0xa3, 0x3e, 0xd5, 0xa8, 0x9e, 0x07, 0x3b, 0x79, 0x93, 0x2c, 0xc8, 0x4c,
	0x8a, 0xb1, 0x26, 0x3a, 0x4d, 0x7a, 0x2f, 0x61, 0x2d, 0xfb, 0x42, 0x6d, 0x68, 0xbe, 0x7e, 0x35,
	0x3e, 0x3f, 0x3e, 0x3c, 0x3d, 0x39, 0x3d, 0x3e, 0xea, 0x2c, 0xa1, 0x26, 0xac, 0x1f, 0xbf, 0xda,
	0x3f, 0x38, 0x3b, 0x3e, 0xea, 0xd4, 0xd0, 0x06, 0x34, 0x8e, 0x4e, 0xc7, 0xd9, 0xaa, 0x6e, 0x06,
	0x85, 0x27, 0x43, 0xa9, 0x4f, 0x03, 0x8f, 0xc6, 0x9c, 0xf9, 0x16, 0xf0, 0x50, 0x8a, 0x09, 0x0b,
	0x53, 0x65, 0x17, 0xe8, 0x0c, 0xd6, 0x12, 0x8b, 0x6b, 0x5d, 0x69, 0x0d, 0x5e, 0xce, 0x4e, 0x38,
	0x1f, 0x54, 0xef, 0x67, 0x36, 0x79, 0x0e, 0xe3, 0x7f, 0xb4, 0xf2, 0x5f, 0x00, 0x0f, 0x7f, 0x57,
	0x9a, 0x43, 0x4b, 0x51, 0xde, 0x9f, 0x19, 0xa0, 0x6c, 0x62, 0x36, 0x07, 0x8f, 0x67, 0xad, 0x2c,
	0x0f, 0x60, 0xde, 0x06, 0x2f, 0x8f, 0x78, 0xdf, 0x41, 0xdb, 0x77, 0x33, 0x69, 0x56, 0x41, 0x89,
	0x1b, 0xe6, 0x3e, 0x9b, 0x05, 0xa9, 0x8e, 0xae, 0x5e, 0xcb, 0xaf, 0x8e, 0xb2, 0x7f, 0x80, 0x6e,
	0xe8, 0x0e, 0x08, 0xcf, 0x22, 0xae, 0x7f, 0x14, 0xe2, 0xc3, 0x5c, 0x7f, 0x66, 0x48, 0xee, 0xc3,
	0x56, 0x56, 0x4b, 0x38, 0xa0, 0x3e, 0x79, 0x8b, 0x27, 0xc4, 0x37, 0xb9, 0xd2, 0xb1, 0xb9, 0xb2,
	0x99, 0xb1, 0x8e, 0x0c, 0xe7, 0xc4, 0x32, 0x50, 0x02, 0x8f, 0x0b, 0x4b, 0x48, 0x29, 0x57, 0xb0,
	0x3b, 0xca, 0x4d, 0x7b, 0x94, 0x7b, 0xb3, 0xd6, 0x2c, 0x4e, 0xaf, 0xfc, 0x1c, 0x77, 0xc2, 0x85,
	0x22, 0xe8, 0x0a, 0x3e, 0x9f, 0x4a, 0x8d, 0x59, 0x80, 0xd5, 0x6d, 0x36, 0x60, 0xbf, 0x9c, 0x0e,
	0x5d, 0x64, 0xc3, 0xf0, 0xf5, 0x27, 0xe5, 0x90, 0xf7, 0x64, 0xfa, 0xc1, 0x0c, 0xfd, 0x35, 0xac,
	0x93, 0xac, 0xfa, 0x6c, 0xf1, 0x34, 0x07, 0x5f, 0xcc, 0xc2, 0xcf, 0x15, 0xa7, 0x19, 0xc1, 0x9d,
	0x0e, 0x1a, 0x43, 0xfb, 0x32, 0x1b, 0x95, 0x71, 0x0e, 0xb3, 0x65, 0x61, 0xbe, 0x9c, 0x85, 0x59,
	0x34, 0x51, 0x0f, 0x97, 0xbc, 0xd6, 0x65, 0x85, 0x87, 0x52, 0x78, 0x94, 0x14, 0xa3, 0x2d, 0x2e,
	0xce, 0x22, 0xc8, 0x86, 0x5b, 0x7b, 0x9b, 0x34, 0x07, 0x2f, 0x66, 0x37, 0xf8, 0x88, 0x69, 0x78,
	0xb8, 0xe4, 0x6d, 0x27, 0x8b, 0xc4, 0xd0, 0x4b, 0x58, 0x99, 0x68, 0xc5, 0x6d, 0x17, 0xbc, 0x23,
	0xdb, 0xaa, 0x53, 0xf4, 0x70, 0xc9, 0xb3, 0xd2, 0x46, 0xcb, 0x8e, 0x89, 0x6b, 0x77, 0x6b, 0x55,
	0x27, 0x5c, 0xa3, 0x65, 0xa4, 0xd1, 0x6f, 0x4b, 0x73, 0x4c, 0xc3, 0x6a, 0xf6, 0x66, 0x35, 0xe7,
	0xe7, 0x45, 0xf3, 0x7a, 0x2a, 0xa6, 0x9d, 0x6f, 0xa1, 0xa1, 0xa2, 0x04, 0xc7, 0x4a, 0xc6, 0xb6,
	0xff, 0xdd, 0x71, 0x72, 0x73, 0x03, 0x95, 0x39, 0x39, 0x95, 0x11, 0xd1, 0x1b, 0xd8, 0xf4, 0xdd,
	0xe0, 0x85, 0x0b, 0x20, 0xb0, 0x40, 0x5f, 0xcd, 0x15, 0xda, 0xa2, 0x09, 0x6d, 0xb8, 0xe4, 0xb5,
	0xfd, 0x2a, 0xd3, 0x00, 0x9b, 0xd6, 0xe8, 0xd2, 0x01, 0x73, 0xdb, 0xc5, 0x9a, 0x77, 0x03, 0x2f,
	0x9c, 0x9a, 0x0c, 0x70, 0x54, 0x65, 0x9a, 0x98, 0x11, 0xd7, 0xca, 0x6c, 0xb7, 0xbd, 0x23, 0x66,
	0xf3, 0xad, 0xce, 0xc4, 0x2c, 0xd7, 0x42, 0x17, 0xd0, 0x89, 0x5d, 0xcb, 0x29, 0xd2, 0xb5, 0x75,
	0xb7, 0x65, 0x0b, 0x5b, 0x93, 0xb1, 0x2c, 0xae, 0x32, 0x0f, 0x36, 0x00, 0x6e, 0x1f, 0xf7, 0xdf,
	0xaf, 0x34, 0x6a, 0x9d, 0xfa, 0xf7, 0x2b, 0x8d, 0x76, 0xa7, 0xd3, 0xfb, 0x77, 0x1d, 0x76, 0x4c,
	0x85, 0xd3, 0x0b, 0xa2, 0x98, 0x19, 0xda, 0xc6, 0x31, 0xf5, 0xd9, 0xc4, 0x95, 0x22, 0x42, 0xb0,
	0x22, 0x48, 0x44, 0x6d, 0x77, 0xb8, 0xe7, 0xd9, 0x6f, 0xf4, 0x27, 0x3b, 0xf8, 0x2b, 0x1c, 0xd0,
	0x09, 0x13, 0x34, 0x70, 0xd7, 0xe9, 0xcf, 0xe6, 0xd3, 0x7d, 0x11, 0x6a, 0xff, 0x75, 0x42, 0xd5,
	0x51, 0xa6, 0x3d, 0x5c, 0xb2, 0x4f, 0x86, 0x7c, 0x89, 0xfe, 0x0c, 0x68, 0xc2, 0x38, 0xc7, 0xd7,
	0x4c, 0x4f, 0x71, 0xf1, 0x36, 0xcf, 0x2a, 0xff, 0x97, 0x9f, 0xb0, 0xc5, 0x09, 0xe3, 0xfc, 0x0d,
	0xd3, 0xd3, 0x43, 0x07, 0x31, 0x5c, 0xf2, 0x3a, 0x93, 0x19, 0xda, 0xce, 0x3e, 0x34, 0x4b, 0xa6,
	0x98, 0xd1, 0x30, 0x26, 0x41, 0x60, 0xee, 0xf5, 0xe2, 0x09, 0x46, 0x78, 0x9a, 0x79, 0x5f, 0xf3,
	0xb6, 0x1c, 0xf3, 0xd4, 0xbd, 0xc6, 0x0c, 0x6b, 0xe7, 0x1b, 0xe8, 0xcc, 0x6e, 0x55, 0x7e, 0xf6,
	0x94, 0xf5, 0xf3, 0x67, 0x8f, 0x55, 0x3c, 0x58, 0x87, 0xd5, 0x34, 0x21, 0x21, 0x3d, 0xf8, 0xf6,
	0x8f, 0xbf, 0x0a, 0x99, 0x9e, 0xa6, 0x97, 0x7d, 0x5f, 0x46, 0xbb, 0xa5, 0xbf, 0x35, 0x77, 0x7f,
	0x86, 0xb2, 0xfa, 0x1b, 0xe7, 0x72, 0xcd, 0x7e, 0xbd, 0xf8, 0x4f, 0x00, 0x00, 0x00, 0xff, 0xff,
	0x68, 0xcb, 0x47, 0x14, 0xe9, 0x11, 0x00, 0x00,
}
